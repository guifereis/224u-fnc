{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import os\n",
    "from tqdm import tqdm, trange, tqdm_notebook\n",
    "import re\n",
    "import nltk\n",
    "from multiprocessing import cpu_count, Pool\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./fnc-1/\"\n",
    "bodies_train = pd.read_csv(os.path.join(data_path, \"train_bodies.csv\"), header=0)\n",
    "headlines_train = pd.read_csv(os.path.join(data_path, \"train_stances.csv\"), header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1683, 2)\n",
      "(49972, 3)\n"
     ]
    }
   ],
   "source": [
    "print(bodies_train.shape)\n",
    "print(headlines_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodies_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_train[headlines_train[\"Body ID\"] == 712]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = headlines_train.merge(bodies_train, on=[\"Body ID\"], how=\"left\")\n",
    "X_train = temp[[\"Headline\", \"articleBody\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPORARY LIMIT JUST FOR TESTING TODO REMOVE\n",
    "#X_train = X_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## START BASELINE FEATURE RE-WRITE\n",
    "_wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "def normalize_word(w):\n",
    "    return _wnl.lemmatize(w).lower()\n",
    "\n",
    "\n",
    "def get_tokenized_lemmas(s):\n",
    "    return [normalize_word(t) for t in nltk.word_tokenize(s)]\n",
    "\n",
    "\n",
    "def clean(s):\n",
    "    # Cleans a string: Lowercasing, trimming, removing non-alphanumeric\n",
    "    return \" \".join(re.findall(r'\\w+', s, flags=re.UNICODE)).lower()\n",
    "\n",
    "\n",
    "def remove_stopwords(l):\n",
    "    # Removes stopwords from a list of tokens\n",
    "    return [w for w in l if w not in feature_extraction.text.ENGLISH_STOP_WORDS]\n",
    "\n",
    "def word_overlap_features_2(df):\n",
    "    clean_headline = set(df[\"___clean_headline_tokenized_lemmas\"])\n",
    "    clean_body = set(df[\"___clean_body_tokenized_lemmas\"])\n",
    "    feature = len(clean_headline & clean_body)/float(len(clean_headline | clean_body))\n",
    "    return feature\n",
    "\n",
    "def refuting_features_adder(df):\n",
    "    # Returns 1/0 if each of these words is present in headline\n",
    "    _refuting_words = [\n",
    "        'fake',\n",
    "        'fraud',\n",
    "        'hoax',\n",
    "        'false',\n",
    "        'deny', 'denies',\n",
    "        # 'refute',\n",
    "        'not',\n",
    "        'despite',\n",
    "        'nope',\n",
    "        'doubt', 'doubts',\n",
    "        'bogus',\n",
    "        'debunk',\n",
    "        'pranks',\n",
    "        'retract'\n",
    "    ]\n",
    "    clean_headline = df[\"___clean_headline_tokenized_lemmas\"]\n",
    "    features = clean_headline.apply(lambda hl: pd.Series([1 if word in hl else 0 for word in _refuting_words]))\n",
    "    features.columns = [\"wrf_hl_\"+ref_word for ref_word in _refuting_words]\n",
    "    return pd.concat([df, features], axis=1)\n",
    "\n",
    "\n",
    "def polarity_features_adder(df):\n",
    "    _refuting_words = [\n",
    "        'fake',\n",
    "        'fraud',\n",
    "        'hoax',\n",
    "        'false',\n",
    "        'deny', 'denies',\n",
    "        'not',\n",
    "        'despite',\n",
    "        'nope',\n",
    "        'doubt', 'doubts',\n",
    "        'bogus',\n",
    "        'debunk',\n",
    "        'pranks',\n",
    "        'retract'\n",
    "    ]\n",
    "    def calculate_polarity(tokens):\n",
    "        return sum([t in _refuting_words for t in tokens]) % 2\n",
    "    \n",
    "    clean_headline = df[\"___clean_headline_tokenized_lemmas\"]\n",
    "    clean_body = df[\"___clean_body_tokenized_lemmas\"]\n",
    "    \n",
    "    \n",
    "    headline_polarity = pd.DataFrame(clean_headline.apply(calculate_polarity))\n",
    "    headline_polarity.columns = [\"polar_hl\"]\n",
    "    \n",
    "    body_polarity = pd.DataFrame(clean_body.apply(calculate_polarity))\n",
    "    body_polarity.columns = [\"polar_body\"]\n",
    "    \n",
    "    df = pd.concat([df, headline_polarity, body_polarity], axis=1)\n",
    "    return df\n",
    "\n",
    "## START hand_features\n",
    "def binary_co_occurrence(row):\n",
    "    # Count how many times a token in the title\n",
    "    # appears in the body text.\n",
    "    bin_count = 0\n",
    "    bin_count_early = 0\n",
    "    for headline_token in row[\"___clean_headline\"].split(\" \"):\n",
    "        if headline_token in row[\"___clean_body\"]:\n",
    "            bin_count += 1\n",
    "        if headline_token in row[\"___clean_body\"][:255]:\n",
    "            bin_count_early += 1\n",
    "    return pd.Series((bin_count, bin_count_early))\n",
    "\n",
    "def binary_co_occurence_stops(row):\n",
    "        # Count how many times a token in the title\n",
    "        # appears in the body text. Stopwords in the title\n",
    "        # are ignored.\n",
    "        bin_count_stopless = 0\n",
    "        #bin_count_early = 0\n",
    "        for headline_token in remove_stopwords(row[\"___clean_headline\"].split(\" \")):\n",
    "            if headline_token in row[\"___clean_body\"]:\n",
    "                bin_count_stopless += 1\n",
    "                #bin_count_early += 1 # TODO why are these values the same in the baseline??? bizarre... deleting\n",
    "        return bin_count_stopless\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chargrams(input, n):\n",
    "    output = []\n",
    "    for i in range(len(input) - n + 1):\n",
    "        output.append(input[i:i + n])\n",
    "    return output\n",
    "\n",
    "def ngrams(input, n):\n",
    "    input = input.split(' ')\n",
    "    output = []\n",
    "    for i in range(len(input) - n + 1):\n",
    "        output.append(input[i:i + n])\n",
    "    return output\n",
    "\n",
    "def append_chargrams(row, size=None):\n",
    "    grams = [' '.join(x) for x in chargrams(\" \".join(remove_stopwords(row[\"___clean_headline\"].split())), size)]\n",
    "    grams_hits = 0\n",
    "    grams_early_hits = 0\n",
    "    grams_first_hits = 0\n",
    "    for gram in grams:\n",
    "        if gram in row[\"___clean_body\"]:\n",
    "            grams_hits += 1\n",
    "        if gram in row[\"___clean_body\"][:255]:\n",
    "            grams_early_hits += 1\n",
    "        if gram in row[\"___clean_body\"][:100]:\n",
    "            grams_first_hits += 1\n",
    "    return pd.Series((grams_hits, grams_early_hits, grams_first_hits))\n",
    "\n",
    "def append_ngrams(row, size=None):\n",
    "    grams = [' '.join(x) for x in ngrams(row[\"___clean_headline\"], size)]\n",
    "    grams_hits = 0\n",
    "    grams_early_hits = 0\n",
    "    for gram in grams:\n",
    "        if gram in row[\"___clean_body\"]:\n",
    "            grams_hits += 1\n",
    "        if gram in row[\"___clean_body\"][:255]:\n",
    "            grams_early_hits += 1\n",
    "    return pd.Series((grams_hits, grams_early_hits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiprocessing code copied from: http://blog.adeel.io/2016/11/06/parallelize-pandas-map-or-apply/\n",
    "cores = cpu_count() #Number of CPU cores on your system\n",
    "partitions = cores #Define as many partitions as you want\n",
    "def parallelize(data, func):\n",
    "    data_split = np.array_split(data, partitions)\n",
    "    pool = Pool(cores)\n",
    "    data = pd.concat(pool.map(func, data_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return data\n",
    "def word_overlap_features_parallel_helper(df):\n",
    "        return df.apply(lambda row: word_overlap_features_2(row), axis=1)\n",
    "def clean_helper(df):\n",
    "        return df.apply(clean)\n",
    "def get_tokenized_lemmas_helper(df):\n",
    "        return df.apply(get_tokenized_lemmas)\n",
    "def chargram_helper(df, input_size=None):\n",
    "    return df.apply(lambda row: append_chargrams(row, input_size), axis=1)\n",
    "def ngram_helper(df, input_size=None):\n",
    "    return df.apply(lambda row: append_ngrams(row, input_size), axis=1)\n",
    "\n",
    "def add_cached_columns(df):\n",
    "    # column names starting with 3 underscores (___....) are cached intermediate\n",
    "    # values only used to speed-up feature computation\n",
    "\n",
    "    df[\"___clean_headline\"] = parallelize(df[\"Headline\"], clean_helper)\n",
    "    df[\"___clean_headline_tokenized_lemmas\"] = parallelize(df[\"___clean_headline\"], get_tokenized_lemmas_helper)\n",
    "    \n",
    "    df[\"___clean_body\"] = parallelize(df[\"articleBody\"], clean_helper)\n",
    "    df[\"___clean_body_tokenized_lemmas\"] = parallelize(df[\"___clean_body\"], get_tokenized_lemmas_helper)\n",
    "    \n",
    "\n",
    "def gen_all_features(df):\n",
    "    print(\"Adding cached columns...\")\n",
    "    add_cached_columns(X_train)\n",
    "    \n",
    "    print(\"Adding co-occurrences...\")\n",
    "    co_occurrences = df.apply(binary_co_occurrence, axis=1)\n",
    "    co_occurrences.columns = [\"bin_count\", \"bin_count_early\"]\n",
    "    df = pd.concat([df, co_occurrences], axis=1)\n",
    "    df[\"bin_count_stopless\"] = df.apply(binary_co_occurence_stops, axis=1)\n",
    "    \n",
    "    # Note: As far as I can tell, the SettingWithCopy warning the following call raises\n",
    "    # is just a false positive. Usage is actually ok.\n",
    "    print(\"Adding word_overlap_features....\")\n",
    "    df[\"word_overlap_features\"] = parallelize(df, word_overlap_features_parallel_helper)\n",
    "    \n",
    "    print(\"Adding refuting_features....\")\n",
    "    df = refuting_features_adder(df)\n",
    "    \n",
    "    print(\"Adding polarity_features....\")\n",
    "    df = polarity_features_adder(df)\n",
    "    \n",
    "    print(\"Adding ngrams...\")\n",
    "    ngram_sizes = [2, 3, 4, 5, 6]\n",
    "    for ngram_size in tqdm_notebook(ngram_sizes, total=len(ngram_sizes)):\n",
    "        helper_fn = partial(ngram_helper, input_size=ngram_size)\n",
    "        temp = parallelize(df, helper_fn)\n",
    "        temp.columns = [\"ngram_\"+str(ngram_size)+\"_hits\", \"ngram_\"+str(ngram_size)+\"_early_hits\"]\n",
    "        df = pd.concat([df, temp], axis=1)\n",
    "    \n",
    "    print(\"Adding chargrams...\")\n",
    "    chargram_sizes = [2, 8, 4, 16]\n",
    "    for chargram_size in tqdm_notebook(chargram_sizes, total=len(chargram_sizes)):\n",
    "        helper_fn = partial(chargram_helper, input_size=chargram_size)\n",
    "        temp = parallelize(df, helper_fn)\n",
    "        temp.columns = [\"chargram_\"+str(chargram_size)+\"_hits\", \"chargram_\"+str(chargram_size)+\"_early_hits\", \"chargram_\"+str(chargram_size)+\"_first_hits\"]\n",
    "        df = pd.concat([df, temp], axis=1)\n",
    "    \n",
    "    return df\n",
    "%time X_train = gen_all_features(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_hdf('X_train_full_allfeatures.h5','df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
