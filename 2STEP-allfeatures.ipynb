{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/gui/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gui/anaconda3/envs/nlu4/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import os\n",
    "from tqdm import tqdm, trange, tqdm_notebook\n",
    "import re\n",
    "import nltk\n",
    "from multiprocessing import cpu_count, Pool\n",
    "from functools import partial\n",
    "import ipywidgets\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from rewrite.scorer import score_4class\n",
    "import utils # utils from CS224U\n",
    "from scipy.spatial import distance\n",
    "import random\n",
    "tqdm.pandas()\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.sentiment import vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_str_cols(df): # df should be X, e.g. X_train or X_dev\n",
    "    del df[\"articleBody\"]\n",
    "    del df[\"Headline\"]\n",
    "    del df[\"Body ID\"]\n",
    "    for col_name in df.columns:\n",
    "        if \"___\" == col_name[0:3]:\n",
    "            del df[col_name]\n",
    "            \n",
    "def print_reports(preds, actual):\n",
    "    print(classification_report(actual, preds))\n",
    "    score, max_score = score_4class(actual, preds)\n",
    "    print(\"Weighted accuracy: \"+str(score/max_score)+\" (\"+str(score)+\" out of \"+str(max_score)+\")\")\n",
    "\n",
    "def print_feature_importances(model, df):\n",
    "    feat_imp = model.feature_importances_\n",
    "    indices = np.argsort(feat_imp)[::-1]\n",
    "    for ii in indices:\n",
    "        print(df.columns[ii]+\": \"+str(feat_imp[ii]))\n",
    "        \n",
    "# Usage: pass Xy_train (or a df with Body ID col., and y col. as well)\n",
    "# returns the same format of df, but spplit so that no body ID is shared between sets\n",
    "proportion_for_holdout_dev_set = 0.2\n",
    "def disjoint_train_test_split(Xy, frac_for_test_set=proportion_for_holdout_dev_set, random_state=42):\n",
    "    r = random.Random()\n",
    "    r.seed(random_state)\n",
    "    Xy_IDs = list(set(Xy[\"Body ID\"]))\n",
    "    print(\"Total unique IDs: \"+str(len(Xy_IDs)))\n",
    "    r.shuffle(Xy_IDs)\n",
    "    num_IDs_for_train = int((1-frac_for_test_set)*len(Xy_IDs))\n",
    "    train_IDs = Xy_IDs[:num_IDs_for_train]\n",
    "    test_IDs = Xy_IDs[num_IDs_for_train:]\n",
    "    train_df = Xy[Xy[\"Body ID\"].isin(train_IDs)]\n",
    "    test_df = Xy[Xy[\"Body ID\"].isin(test_IDs)]\n",
    "    print(\"# instances in train: \"+str(train_df.shape[0]))\n",
    "    print(\"# instances in test: \"+str(test_df.shape[0]))\n",
    "    assert len(set(train_df[\"Body ID\"].unique()) & set(test_df[\"Body ID\"].unique())) == 0 # totally disjoint\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoModel:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        #self.make_other_sets()\n",
    "        \n",
    "    def make_other_sets(self):\n",
    "        #Xy_train = pd.concat([X_train, y_train], axis=1)\n",
    "        Xy_train = self.X.merge(self.y, on=[\"Body ID\"], how=\"left\")\n",
    "        #Xy_train, Xy_dev = sklearn.model_selection.train_test_split(Xy_train, test_size=0.2, random_state=42, shuffle=True)\n",
    "        Xy_train, Xy_dev = disjoint_train_test_split(Xy_train, proportion_for_holdout_dev_set, random_state=43)\n",
    "        \n",
    "        Xy_train_1 = Xy_train.copy()\n",
    "        Xy_train_1.loc[Xy_train_1[\"Stance\"] != \"unrelated\", 'Stance'] = \"related\"\n",
    "        self.y_train_1 = Xy_train_1[\"Stance\"]\n",
    "        self.X_train_1 = Xy_train_1.drop(\"Stance\", axis=1)\n",
    "        \n",
    "        Xy_dev_1 = Xy_dev.copy()\n",
    "        Xy_dev_1.loc[Xy_dev_1[\"Stance\"] != \"unrelated\", 'Stance'] = \"related\"\n",
    "        self.y_dev_1 = Xy_dev_1[\"Stance\"]\n",
    "        self.X_dev_1 = Xy_dev_1.drop(\"Stance\", axis=1)\n",
    "        \n",
    "        Xy_train_2 = Xy_train[Xy_train[\"Stance\"] != \"unrelated\"]\n",
    "        self.y_train_2 = Xy_train_2[\"Stance\"]\n",
    "        self.X_train_2 = Xy_train_2.drop(\"Stance\", axis=1)\n",
    "\n",
    "        Xy_dev_2 = Xy_dev[Xy_dev[\"Stance\"] != \"unrelated\"]\n",
    "        self.y_dev_2 = Xy_dev_2[\"Stance\"]\n",
    "        self.X_dev_2 = Xy_dev_2.drop(\"Stance\", axis=1)\n",
    "        \n",
    "        del_str_cols(self.X_train_1)\n",
    "        del_str_cols(self.X_dev_1)\n",
    "        del_str_cols(self.X_train_2)\n",
    "        del_str_cols(self.X_dev_2)\n",
    "        \n",
    "        return Xy_train.drop(\"Stance\", axis=1), Xy_train[\"Stance\"], Xy_dev.drop(\"Stance\", axis=1), Xy_dev[\"Stance\"]\n",
    "        \n",
    "    def fit(self):\n",
    "        ## CLASSIFIER 1 - RELATED/UNRELATED\n",
    "        self.mod1 = GradientBoostingClassifier(n_estimators=200, random_state=14128, verbose=True)\n",
    "        self.mod1.fit(self.X_train_1, self.y_train_1)\n",
    "        \n",
    "        ## CLASSIFIER 2 - 3-class\n",
    "        self.mod2 = GradientBoostingClassifier(n_estimators=200, random_state=14128, verbose=True)\n",
    "        self.mod2.fit(self.X_train_2, self.y_train_2)\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        #if X is None:\n",
    "        #    X = self.X_dev_1\n",
    "        del_str_cols(X)\n",
    "        preds_1 = self.mod1.predict(X)\n",
    "        preds_2 = self.mod2.predict(X) # note X_dev_1\n",
    "        \n",
    "        new_preds = preds_1.copy()\n",
    "        for ii in range(preds_1.shape[0]):\n",
    "            if preds_1[ii] == \"related\":\n",
    "                new_preds[ii] = preds_2[ii]\n",
    "            else:\n",
    "                new_preds[ii] = \"unrelated\"\n",
    "        return new_preds\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count, Pool\n",
    "cores = cpu_count() \n",
    "partitions = cores\n",
    "def parallelize(data, func):\n",
    "    data_split = np.array_split(data, partitions)\n",
    "    pool = Pool(cores)\n",
    "    data = pd.concat(pool.map(func, data_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ADD OUR FEATURES\n",
    "# START SENTIMENT ANALYSIS\n",
    "def vader_pol_helper(df):\n",
    "    return df.apply(lambda hl: pd.Series(sid.polarity_scores(hl)))\n",
    "sid = vader.SentimentIntensityAnalyzer() # global scope for parallelization\n",
    "def add_vader_sent(X_df):\n",
    "    def vader_polarity_scores(df, text_col_name, col_name_prefix):\n",
    "        pol_scores = parallelize(df[text_col_name], vader_pol_helper)\n",
    "        #pol_scores = df[text_col_name].progress_apply(lambda hl: pd.Series(sid.polarity_scores(hl)))\n",
    "        cols = pol_scores.columns\n",
    "        new_cols = []\n",
    "        for col_name in cols:\n",
    "            new_cols.append(\"vader_\"+col_name_prefix+\"_\"+col_name)\n",
    "        pol_scores.columns = new_cols\n",
    "        return pol_scores\n",
    "\n",
    "    vader_hl_df = vader_polarity_scores(X_df, \"Headline\", \"hl\")\n",
    "    vader_body_df = vader_polarity_scores(X_df, \"articleBody\", \"body\")\n",
    "    X_df = pd.concat([X_df, vader_hl_df, vader_body_df], axis=1)\n",
    "    return X_df\n",
    "\n",
    "# END SENTIMENT ANALYSIS\n",
    "### GLOVE ####\n",
    "glove_dim = 200\n",
    "glove_src = os.path.join(\"GloVe\", 'glove.6B.'+str(glove_dim)+'d.txt')\n",
    "GLOVE = utils.glove2dict(glove_src)\n",
    "def text_to_mean_vec_ignore_unk(text, w2v=GLOVE, dim=glove_dim):\n",
    "    vec = np.zeros(dim)\n",
    "    num_added = 0\n",
    "    for word in text:\n",
    "        if word in w2v:\n",
    "            vec += w2v[word]\n",
    "            num_added += 1\n",
    "    if num_added > 0:\n",
    "        return vec/num_added\n",
    "    else:\n",
    "        return np.array([random.uniform(-0.5, 0.5) for i in range(glove_dim)])\n",
    "def get_glove_cos_dist_hl_body(row):\n",
    "    hl = row[\"___clean_headline_tokenized_lemmas\"]\n",
    "    body = row[\"___clean_body_tokenized_lemmas\"]\n",
    "    hl_vec = text_to_mean_vec_ignore_unk(hl)\n",
    "    body_vec = text_to_mean_vec_ignore_unk(body)\n",
    "    cosine_dist = distance.cosine(hl_vec, body_vec) # cosine() from scipy\n",
    "    return cosine_dist\n",
    "\n",
    "def get_verbs(text):\n",
    "    verbs = [token for token, pos in nltk.pos_tag(text) if pos.startswith('VB')]\n",
    "    verbs_sentence = ' '.join(word[0] for word in verbs)\n",
    "    return verbs_sentence\n",
    " \n",
    "def get_verb_glove_cos_dist_hl_body(row):\n",
    "    hl = row[\"___clean_headline_tokenized_lemmas\"]\n",
    "    body = row[\"___clean_body_tokenized_lemmas\"]\n",
    "    \n",
    "    hl_verbs = get_verbs(hl)\n",
    "    body_verbs = get_verbs(body)\n",
    "    \n",
    "    hl_vec = text_to_mean_vec_ignore_unk(hl_verbs)\n",
    "    body_vec = text_to_mean_vec_ignore_unk(body_verbs)\n",
    "    cosine_dist = distance.cosine(hl_vec, body_vec) # cosine() from scipy\n",
    "    return cosine_dist\n",
    "\n",
    "def hl_body_glove_cos_dist_helper(X_df):\n",
    "    return X_df[[\"___clean_headline_tokenized_lemmas\", \"___clean_body_tokenized_lemmas\"]].apply(get_glove_cos_dist_hl_body, axis=1)\n",
    "\n",
    "def hl_body_glove_verb_helper(X_df):\n",
    "    return X_df[[\"___clean_headline_tokenized_lemmas\", \"___clean_body_tokenized_lemmas\"]].apply(get_verb_glove_cos_dist_hl_body, axis=1)\n",
    "\n",
    "### END GLOVE CODE BLOCK\n",
    "def add_all_features(X_df, parallel=True):\n",
    "    print(\"Adding glove feature #1...\", flush=True)\n",
    "    X_df[\"hl_body_glove_\"+str(glove_dim)+\"_cos_dist\"] = parallelize(X_df, hl_body_glove_cos_dist_helper)\n",
    "    print(\"Adding glove feature #2...\", flush=True)\n",
    "    X_df[\"hl_body_verb_glove_\"+str(glove_dim)+\"_cos_dist\"] = parallelize(X_df, hl_body_glove_verb_helper)\n",
    "    print(\"Adding VADER sentiment...\", flush=True)\n",
    "    X_df = add_vader_sent(X_df)\n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_hdf(\"X_train_disjoint-allfeatures.h5\", key=\"df\")\n",
    "y_train = pd.read_hdf(\"y_train_disjoint.h5\", key=\"df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding glove feature #1...\n",
      "Adding glove feature #2...\n",
      "Adding VADER sentiment...\n",
      "CPU times: user 9.84 s, sys: 9.88 s, total: 19.7 s\n",
      "Wall time: 5min 37s\n"
     ]
    }
   ],
   "source": [
    "%time X_train = add_all_features(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39437, 60)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39437, 2)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsm = TwoModel(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique IDs: 1346\n",
      "# instances in train: 1752859\n",
      "# instances in test: 481302\n",
      "(1752859, 60)\n",
      "(1752859,)\n",
      "(481302, 60)\n",
      "(481302,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_dev, y_dev = tsm.make_other_sets()\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_dev.shape)\n",
    "print(y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>articleBody</th>\n",
       "      <th>Body ID</th>\n",
       "      <th>___clean_headline</th>\n",
       "      <th>___clean_headline_tokenized_lemmas</th>\n",
       "      <th>___clean_body</th>\n",
       "      <th>___clean_body_tokenized_lemmas</th>\n",
       "      <th>bin_count</th>\n",
       "      <th>bin_count_early</th>\n",
       "      <th>bin_count_stopless</th>\n",
       "      <th>...</th>\n",
       "      <th>hl_body_glove_200_cos_dist</th>\n",
       "      <th>hl_body_verb_glove_200_cos_dist</th>\n",
       "      <th>vader_hl_neg</th>\n",
       "      <th>vader_hl_neu</th>\n",
       "      <th>vader_hl_pos</th>\n",
       "      <th>vader_hl_compound</th>\n",
       "      <th>vader_body_neg</th>\n",
       "      <th>vader_body_neu</th>\n",
       "      <th>vader_body_pos</th>\n",
       "      <th>vader_body_compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Police find mass graves with at least '15 bodi...</td>\n",
       "      <td>Danny Boyle is directing the untitled film\\n\\n...</td>\n",
       "      <td>712</td>\n",
       "      <td>police find mass graves with at least 15 bodie...</td>\n",
       "      <td>[police, find, mass, graf, with, at, least, 15...</td>\n",
       "      <td>danny boyle is directing the untitled film set...</td>\n",
       "      <td>[danny, boyle, is, directing, the, untitled, f...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.157001</td>\n",
       "      <td>0.221717</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.4767</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.9409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Police find mass graves with at least '15 bodi...</td>\n",
       "      <td>Danny Boyle is directing the untitled film\\n\\n...</td>\n",
       "      <td>712</td>\n",
       "      <td>police find mass graves with at least 15 bodie...</td>\n",
       "      <td>[police, find, mass, graf, with, at, least, 15...</td>\n",
       "      <td>danny boyle is directing the untitled film set...</td>\n",
       "      <td>[danny, boyle, is, directing, the, untitled, f...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.157001</td>\n",
       "      <td>0.221717</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.4767</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.9409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Police find mass graves with at least '15 bodi...</td>\n",
       "      <td>Danny Boyle is directing the untitled film\\n\\n...</td>\n",
       "      <td>712</td>\n",
       "      <td>police find mass graves with at least 15 bodie...</td>\n",
       "      <td>[police, find, mass, graf, with, at, least, 15...</td>\n",
       "      <td>danny boyle is directing the untitled film set...</td>\n",
       "      <td>[danny, boyle, is, directing, the, untitled, f...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.157001</td>\n",
       "      <td>0.221717</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.4767</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.9409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Police find mass graves with at least '15 bodi...</td>\n",
       "      <td>Danny Boyle is directing the untitled film\\n\\n...</td>\n",
       "      <td>712</td>\n",
       "      <td>police find mass graves with at least 15 bodie...</td>\n",
       "      <td>[police, find, mass, graf, with, at, least, 15...</td>\n",
       "      <td>danny boyle is directing the untitled film set...</td>\n",
       "      <td>[danny, boyle, is, directing, the, untitled, f...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.157001</td>\n",
       "      <td>0.221717</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.4767</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.9409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Police find mass graves with at least '15 bodi...</td>\n",
       "      <td>Danny Boyle is directing the untitled film\\n\\n...</td>\n",
       "      <td>712</td>\n",
       "      <td>police find mass graves with at least 15 bodie...</td>\n",
       "      <td>[police, find, mass, graf, with, at, least, 15...</td>\n",
       "      <td>danny boyle is directing the untitled film set...</td>\n",
       "      <td>[danny, boyle, is, directing, the, untitled, f...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.157001</td>\n",
       "      <td>0.221717</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.4767</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.9409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  \\\n",
       "0  Police find mass graves with at least '15 bodi...   \n",
       "1  Police find mass graves with at least '15 bodi...   \n",
       "2  Police find mass graves with at least '15 bodi...   \n",
       "3  Police find mass graves with at least '15 bodi...   \n",
       "4  Police find mass graves with at least '15 bodi...   \n",
       "\n",
       "                                         articleBody  Body ID  \\\n",
       "0  Danny Boyle is directing the untitled film\\n\\n...      712   \n",
       "1  Danny Boyle is directing the untitled film\\n\\n...      712   \n",
       "2  Danny Boyle is directing the untitled film\\n\\n...      712   \n",
       "3  Danny Boyle is directing the untitled film\\n\\n...      712   \n",
       "4  Danny Boyle is directing the untitled film\\n\\n...      712   \n",
       "\n",
       "                                   ___clean_headline  \\\n",
       "0  police find mass graves with at least 15 bodie...   \n",
       "1  police find mass graves with at least 15 bodie...   \n",
       "2  police find mass graves with at least 15 bodie...   \n",
       "3  police find mass graves with at least 15 bodie...   \n",
       "4  police find mass graves with at least 15 bodie...   \n",
       "\n",
       "                  ___clean_headline_tokenized_lemmas  \\\n",
       "0  [police, find, mass, graf, with, at, least, 15...   \n",
       "1  [police, find, mass, graf, with, at, least, 15...   \n",
       "2  [police, find, mass, graf, with, at, least, 15...   \n",
       "3  [police, find, mass, graf, with, at, least, 15...   \n",
       "4  [police, find, mass, graf, with, at, least, 15...   \n",
       "\n",
       "                                       ___clean_body  \\\n",
       "0  danny boyle is directing the untitled film set...   \n",
       "1  danny boyle is directing the untitled film set...   \n",
       "2  danny boyle is directing the untitled film set...   \n",
       "3  danny boyle is directing the untitled film set...   \n",
       "4  danny boyle is directing the untitled film set...   \n",
       "\n",
       "                      ___clean_body_tokenized_lemmas  bin_count  \\\n",
       "0  [danny, boyle, is, directing, the, untitled, f...          2   \n",
       "1  [danny, boyle, is, directing, the, untitled, f...          2   \n",
       "2  [danny, boyle, is, directing, the, untitled, f...          2   \n",
       "3  [danny, boyle, is, directing, the, untitled, f...          2   \n",
       "4  [danny, boyle, is, directing, the, untitled, f...          2   \n",
       "\n",
       "   bin_count_early  bin_count_stopless         ...           \\\n",
       "0                0                   0         ...            \n",
       "1                0                   0         ...            \n",
       "2                0                   0         ...            \n",
       "3                0                   0         ...            \n",
       "4                0                   0         ...            \n",
       "\n",
       "   hl_body_glove_200_cos_dist  hl_body_verb_glove_200_cos_dist  vader_hl_neg  \\\n",
       "0                    0.157001                         0.221717         0.194   \n",
       "1                    0.157001                         0.221717         0.194   \n",
       "2                    0.157001                         0.221717         0.194   \n",
       "3                    0.157001                         0.221717         0.194   \n",
       "4                    0.157001                         0.221717         0.194   \n",
       "\n",
       "   vader_hl_neu  vader_hl_pos  vader_hl_compound  vader_body_neg  \\\n",
       "0         0.806           0.0            -0.4767           0.008   \n",
       "1         0.806           0.0            -0.4767           0.008   \n",
       "2         0.806           0.0            -0.4767           0.008   \n",
       "3         0.806           0.0            -0.4767           0.008   \n",
       "4         0.806           0.0            -0.4767           0.008   \n",
       "\n",
       "   vader_body_neu  vader_body_pos  vader_body_compound  \n",
       "0           0.907           0.085               0.9409  \n",
       "1           0.907           0.085               0.9409  \n",
       "2           0.907           0.085               0.9409  \n",
       "3           0.907           0.085               0.9409  \n",
       "4           0.907           0.085               0.9409  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsm.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = tsm.predict(X_dev)\n",
    "print_reports(preds, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_hdf(\"X_TEST_full_allfeatures-NOLABEL.h5\", key=\"df\")\n",
    "y_test = pd.read_hdf(\"y_TEST_full.h5\", key=\"df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = add_all_features(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = tsm.predict(X_test)\n",
    "print_reports(preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = sklearn.ensemble.RandomForestClassifier(n_estimators=200, n_jobs=8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = crf.predict(X_dev)\n",
    "print(classification_report(y_dev, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, preds, labels=[\"agree\", \"disagree\", \"discuss\"], sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
