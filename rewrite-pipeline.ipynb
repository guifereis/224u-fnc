{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import os\n",
    "from tqdm import tqdm, trange, tqdm_notebook\n",
    "import re\n",
    "import nltk\n",
    "from multiprocessing import cpu_count, Pool\n",
    "from functools import partial\n",
    "import ipywidgets\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script will:\n",
    "# i) Read in train and test .csv files\n",
    "# ii) From the train files, create a train set and a dev set. These will share no Body IDs (articles) in common.\n",
    "# ii) Re-create the features present in the official fnc-1-baseline for all sets (train, dev/holdout, and test), and\n",
    "#     write all these out as an .h5. You can then load these to build more features/models upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./fnc-1/\"\n",
    "bodies_train = pd.read_csv(os.path.join(data_path, \"train_bodies.csv\"), header=0)\n",
    "headlines_train = pd.read_csv(os.path.join(data_path, \"train_stances.csv\"), header=0)\n",
    "proportion_for_holdout_dev_set = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1683, 2)\n",
      "(49972, 3)\n"
     ]
    }
   ],
   "source": [
    "print(bodies_train.shape)\n",
    "print(headlines_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49972, 4)\n"
     ]
    }
   ],
   "source": [
    "Xy_train = headlines_train.merge(bodies_train, on=[\"Body ID\"], how=\"left\")\n",
    "print(Xy_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage: pass Xy_train (or a df with Body ID col., and y col. as well)\n",
    "# returns the same format of df, but spplit so that no body ID is shared between sets\n",
    "def disjoint_train_test_split(Xy, frac_for_test_set=proportion_for_holdout_dev_set, random_state=42):\n",
    "    r = random.Random()\n",
    "    r.seed(random_state)\n",
    "    Xy_IDs = list(set(Xy[\"Body ID\"]))\n",
    "    print(\"Total unique IDs: \"+str(len(Xy_IDs)))\n",
    "    r.shuffle(Xy_IDs)\n",
    "    num_IDs_for_train = int((1-frac_for_test_set)*len(Xy_IDs))\n",
    "    train_IDs = Xy_IDs[:num_IDs_for_train]\n",
    "    test_IDs = Xy_IDs[num_IDs_for_train:]\n",
    "    train_df = Xy_train[Xy_train[\"Body ID\"].isin(train_IDs)]\n",
    "    test_df = Xy_train[Xy_train[\"Body ID\"].isin(test_IDs)]\n",
    "    print(\"# instances in train: \"+str(train_df.shape[0]))\n",
    "    print(\"# instances in test: \"+str(test_df.shape[0]))\n",
    "    assert len(set(train_df[\"Body ID\"].unique()) & set(test_df[\"Body ID\"].unique())) == 0 # totally disjoint\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique IDs: 1683\n",
      "# instances in train: 39437\n",
      "# instances in test: 10535\n"
     ]
    }
   ],
   "source": [
    "Xy_train, Xy_dev = disjoint_train_test_split(Xy_train, frac_for_test_set=proportion_for_holdout_dev_set)\n",
    "# We will also keep the Body ID in both df's in case we want to merge them back when modelling\n",
    "# (e.g. to filter out a specific class), and they have been shuffled.\n",
    "X_train = Xy_train[[\"Headline\", \"articleBody\", \"Body ID\"]]\n",
    "y_train = Xy_train[[\"Stance\", \"Body ID\"]]\n",
    "\n",
    "X_dev = Xy_dev[[\"Headline\", \"articleBody\", \"Body ID\"]]\n",
    "y_dev = Xy_dev[[\"Stance\", \"Body ID\"]]\n",
    "\n",
    "y_train.to_hdf('y_train_disjoint.h5','df')\n",
    "y_dev.to_hdf('y_dev_disjoint.h5','df')\n",
    "del Xy_train # purposefully make these not accessibly so we do not risk leaking data from y\n",
    "del Xy_dev\n",
    "del y_train\n",
    "del y_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We re-create the features from the official baseline for X_train and then X_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions. You should call gen_all_features(X_df) to create the features.\n",
    "# You will need approx. 14GB of memory since we cache columns and copy the strings rather than referring to them\n",
    "# by ID.\n",
    "_wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "def normalize_word(w):\n",
    "    return _wnl.lemmatize(w).lower()\n",
    "\n",
    "\n",
    "def get_tokenized_lemmas(s):\n",
    "    return [normalize_word(t) for t in nltk.word_tokenize(s)]\n",
    "\n",
    "\n",
    "def clean(s):\n",
    "    # Cleans a string: Lowercasing, trimming, removing non-alphanumeric\n",
    "    return \" \".join(re.findall(r'\\w+', s, flags=re.UNICODE)).lower()\n",
    "\n",
    "\n",
    "def remove_stopwords(l):\n",
    "    # Removes stopwords from a list of tokens\n",
    "    return [w for w in l if w not in sklearn.feature_extraction.text.ENGLISH_STOP_WORDS]\n",
    "\n",
    "def word_overlap_features_2(df):\n",
    "    clean_headline = set(df[\"___clean_headline_tokenized_lemmas\"])\n",
    "    clean_body = set(df[\"___clean_body_tokenized_lemmas\"])\n",
    "    feature = len(clean_headline & clean_body)/float(len(clean_headline | clean_body))\n",
    "    return feature\n",
    "\n",
    "def refuting_features_adder(df):\n",
    "    # Returns 1/0 if each of these words is present in headline\n",
    "    _refuting_words = [\n",
    "        'fake',\n",
    "        'fraud',\n",
    "        'hoax',\n",
    "        'false',\n",
    "        'deny', 'denies',\n",
    "        # 'refute',\n",
    "        'not',\n",
    "        'despite',\n",
    "        'nope',\n",
    "        'doubt', 'doubts',\n",
    "        'bogus',\n",
    "        'debunk',\n",
    "        'pranks',\n",
    "        'retract'\n",
    "    ]\n",
    "    clean_headline = df[\"___clean_headline_tokenized_lemmas\"]\n",
    "    features = clean_headline.apply(lambda hl: pd.Series([1 if word in hl else 0 for word in _refuting_words]))\n",
    "    features.columns = [\"wrf_hl_\"+ref_word for ref_word in _refuting_words]\n",
    "    return pd.concat([df, features], axis=1)\n",
    "\n",
    "\n",
    "def polarity_features_adder(df):\n",
    "    _refuting_words = [\n",
    "        'fake',\n",
    "        'fraud',\n",
    "        'hoax',\n",
    "        'false',\n",
    "        'deny', 'denies',\n",
    "        'not',\n",
    "        'despite',\n",
    "        'nope',\n",
    "        'doubt', 'doubts',\n",
    "        'bogus',\n",
    "        'debunk',\n",
    "        'pranks',\n",
    "        'retract'\n",
    "    ]\n",
    "    def calculate_polarity(tokens):\n",
    "        return sum([t in _refuting_words for t in tokens]) % 2\n",
    "    \n",
    "    clean_headline = df[\"___clean_headline_tokenized_lemmas\"]\n",
    "    clean_body = df[\"___clean_body_tokenized_lemmas\"]\n",
    "    \n",
    "    \n",
    "    headline_polarity = pd.DataFrame(clean_headline.apply(calculate_polarity))\n",
    "    headline_polarity.columns = [\"polar_hl\"]\n",
    "    \n",
    "    body_polarity = pd.DataFrame(clean_body.apply(calculate_polarity))\n",
    "    body_polarity.columns = [\"polar_body\"]\n",
    "    \n",
    "    df = pd.concat([df, headline_polarity, body_polarity], axis=1)\n",
    "    return df\n",
    "\n",
    "## START hand_features\n",
    "def binary_co_occurrence(row):\n",
    "    # Count how many times a token in the title\n",
    "    # appears in the body text.\n",
    "    bin_count = 0\n",
    "    bin_count_early = 0\n",
    "    for headline_token in row[\"___clean_headline\"].split(\" \"):\n",
    "        if headline_token in row[\"___clean_body\"]:\n",
    "            bin_count += 1\n",
    "        if headline_token in row[\"___clean_body\"][:255]:\n",
    "            bin_count_early += 1\n",
    "    return pd.Series((bin_count, bin_count_early))\n",
    "\n",
    "def binary_co_occurence_stops(row):\n",
    "        # Count how many times a token in the title\n",
    "        # appears in the body text. Stopwords in the title\n",
    "        # are ignored.\n",
    "        bin_count_stopless = 0\n",
    "        #bin_count_early = 0\n",
    "        for headline_token in remove_stopwords(row[\"___clean_headline\"].split(\" \")):\n",
    "            if headline_token in row[\"___clean_body\"]:\n",
    "                bin_count_stopless += 1\n",
    "                #bin_count_early += 1 # This is technically in the baseline, but it add no new information, so not including\n",
    "        return bin_count_stopless\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chargrams(input, n):\n",
    "    output = []\n",
    "    for i in range(len(input) - n + 1):\n",
    "        output.append(input[i:i + n])\n",
    "    return output\n",
    "\n",
    "def ngrams(input, n):\n",
    "    input = input.split(' ')\n",
    "    output = []\n",
    "    for i in range(len(input) - n + 1):\n",
    "        output.append(input[i:i + n])\n",
    "    return output\n",
    "\n",
    "def append_chargrams(row, size=None):\n",
    "    grams = [' '.join(x) for x in chargrams(\" \".join(remove_stopwords(row[\"___clean_headline\"].split())), size)]\n",
    "    grams_hits = 0\n",
    "    grams_early_hits = 0\n",
    "    grams_first_hits = 0\n",
    "    for gram in grams:\n",
    "        if gram in row[\"___clean_body\"]:\n",
    "            grams_hits += 1\n",
    "        if gram in row[\"___clean_body\"][:255]:\n",
    "            grams_early_hits += 1\n",
    "        if gram in row[\"___clean_body\"][:100]:\n",
    "            grams_first_hits += 1\n",
    "    return pd.Series((grams_hits, grams_early_hits, grams_first_hits))\n",
    "\n",
    "def append_ngrams(row, size=None):\n",
    "    grams = [' '.join(x) for x in ngrams(row[\"___clean_headline\"], size)]\n",
    "    grams_hits = 0\n",
    "    grams_early_hits = 0\n",
    "    for gram in grams:\n",
    "        if gram in row[\"___clean_body\"]:\n",
    "            grams_hits += 1\n",
    "        if gram in row[\"___clean_body\"][:255]:\n",
    "            grams_early_hits += 1\n",
    "    return pd.Series((grams_hits, grams_early_hits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiprocessing code copied from: http://blog.adeel.io/2016/11/06/parallelize-pandas-map-or-apply/\n",
    "cores = cpu_count() #Number of CPU cores on your system\n",
    "partitions = cores #Define as many partitions as you want\n",
    "def parallelize(data, func):\n",
    "    data_split = np.array_split(data, partitions)\n",
    "    pool = Pool(cores)\n",
    "    data = pd.concat(pool.map(func, data_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return data\n",
    "def word_overlap_features_parallel_helper(df):\n",
    "        return df.apply(lambda row: word_overlap_features_2(row), axis=1)\n",
    "def clean_helper(df):\n",
    "        return df.apply(clean)\n",
    "def get_tokenized_lemmas_helper(df):\n",
    "        return df.apply(get_tokenized_lemmas)\n",
    "def chargram_helper(df, input_size=None):\n",
    "    return df.apply(lambda row: append_chargrams(row, input_size), axis=1)\n",
    "def ngram_helper(df, input_size=None):\n",
    "    return df.apply(lambda row: append_ngrams(row, input_size), axis=1)\n",
    "\n",
    "def add_cached_columns(df):\n",
    "    # column names starting with 3 underscores (___....) are cached intermediate\n",
    "    # values only used to speed-up feature computation\n",
    "    df[\"___clean_headline\"] = parallelize(df[\"Headline\"], clean_helper)\n",
    "    df[\"___clean_headline_tokenized_lemmas\"] = parallelize(df[\"___clean_headline\"], get_tokenized_lemmas_helper)\n",
    "    \n",
    "    df[\"___clean_body\"] = parallelize(df[\"articleBody\"], clean_helper)\n",
    "    df[\"___clean_body_tokenized_lemmas\"] = parallelize(df[\"___clean_body\"], get_tokenized_lemmas_helper)\n",
    "    \n",
    "\n",
    "def gen_all_features(df):\n",
    "    print(\"Adding cached columns...\")\n",
    "    add_cached_columns(df)\n",
    "    \n",
    "    print(\"Adding co-occurrences...\")\n",
    "    co_occurrences = df.apply(binary_co_occurrence, axis=1)\n",
    "    co_occurrences.columns = [\"bin_count\", \"bin_count_early\"]\n",
    "    df = pd.concat([df, co_occurrences], axis=1)\n",
    "    df[\"bin_count_stopless\"] = df.apply(binary_co_occurence_stops, axis=1)\n",
    "    \n",
    "    # Note: As far as I can tell, the SettingWithCopy warning the following call raises\n",
    "    # is just a false positive. Usage is actually ok.\n",
    "    print(\"Adding word_overlap_features....\")\n",
    "    df[\"word_overlap_features\"] = parallelize(df, word_overlap_features_parallel_helper)\n",
    "    \n",
    "    print(\"Adding refuting_features....\")\n",
    "    df = refuting_features_adder(df)\n",
    "    \n",
    "    print(\"Adding polarity_features....\")\n",
    "    df = polarity_features_adder(df)\n",
    "    \n",
    "    print(\"Adding ngrams...\")\n",
    "    ngram_sizes = [2, 3, 4, 5, 6]\n",
    "    for ngram_size in tqdm_notebook(ngram_sizes, total=len(ngram_sizes)):\n",
    "        helper_fn = partial(ngram_helper, input_size=ngram_size)\n",
    "        temp = parallelize(df, helper_fn)\n",
    "        temp.columns = [\"ngram_\"+str(ngram_size)+\"_hits\", \"ngram_\"+str(ngram_size)+\"_early_hits\"]\n",
    "        df = pd.concat([df, temp], axis=1)\n",
    "    \n",
    "    print(\"Adding chargrams...\")\n",
    "    chargram_sizes = [2, 8, 4, 16]\n",
    "    for chargram_size in tqdm_notebook(chargram_sizes, total=len(chargram_sizes)):\n",
    "        helper_fn = partial(chargram_helper, input_size=chargram_size)\n",
    "        temp = parallelize(df, helper_fn)\n",
    "        temp.columns = [\"chargram_\"+str(chargram_size)+\"_hits\", \"chargram_\"+str(chargram_size)+\"_early_hits\", \"chargram_\"+str(chargram_size)+\"_first_hits\"]\n",
    "        df = pd.concat([df, temp], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding cached columns...\n",
      "Adding co-occurrences...\n",
      "Adding word_overlap_features....\n",
      "Adding refuting_features....\n",
      "Adding polarity_features....\n",
      "Adding ngrams...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d704f68aeec4651b231fc48121ba35e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding chargrams...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adec91f44fb54fd186bb293e7b3ffc02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 1min 23s, sys: 2min 35s, total: 3min 59s\n",
      "Wall time: 11min 32s\n"
     ]
    }
   ],
   "source": [
    "%time X_train = gen_all_features(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding cached columns...\n",
      "Adding co-occurrences...\n",
      "Adding word_overlap_features....\n",
      "Adding refuting_features....\n",
      "Adding polarity_features....\n",
      "Adding ngrams...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "142c854b9aa74d7eb0004d544d0bdd47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding chargrams...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "232c7463ec0144f88d73580eb202e32e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 24.2 s, sys: 3min 8s, total: 3min 32s\n",
      "Wall time: 4min 21s\n"
     ]
    }
   ],
   "source": [
    "%time X_dev = gen_all_features(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>articleBody</th>\n",
       "      <th>Body ID</th>\n",
       "      <th>___clean_headline</th>\n",
       "      <th>___clean_headline_tokenized_lemmas</th>\n",
       "      <th>___clean_body</th>\n",
       "      <th>___clean_body_tokenized_lemmas</th>\n",
       "      <th>bin_count</th>\n",
       "      <th>bin_count_early</th>\n",
       "      <th>bin_count_stopless</th>\n",
       "      <th>...</th>\n",
       "      <th>chargram_2_first_hits</th>\n",
       "      <th>chargram_8_hits</th>\n",
       "      <th>chargram_8_early_hits</th>\n",
       "      <th>chargram_8_first_hits</th>\n",
       "      <th>chargram_4_hits</th>\n",
       "      <th>chargram_4_early_hits</th>\n",
       "      <th>chargram_4_first_hits</th>\n",
       "      <th>chargram_16_hits</th>\n",
       "      <th>chargram_16_early_hits</th>\n",
       "      <th>chargram_16_first_hits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Police find mass graves with at least '15 bodi...</td>\n",
       "      <td>Danny Boyle is directing the untitled film\\n\\n...</td>\n",
       "      <td>712</td>\n",
       "      <td>police find mass graves with at least 15 bodie...</td>\n",
       "      <td>[police, find, mass, graf, with, at, least, 15...</td>\n",
       "      <td>danny boyle is directing the untitled film set...</td>\n",
       "      <td>[danny, boyle, is, directing, the, untitled, f...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Christian Bale passes on role of Steve Jobs, a...</td>\n",
       "      <td>30-year-old Moscow resident was hospitalized w...</td>\n",
       "      <td>137</td>\n",
       "      <td>christian bale passes on role of steve jobs ac...</td>\n",
       "      <td>[christian, bale, pass, on, role, of, steve, j...</td>\n",
       "      <td>30 year old moscow resident was hospitalized w...</td>\n",
       "      <td>[30, year, old, moscow, resident, wa, hospital...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>'Nasa Confirms Earth Will Experience 6 Days of...</td>\n",
       "      <td>Thousands of people have been duped by a fake ...</td>\n",
       "      <td>154</td>\n",
       "      <td>nasa confirms earth will experience 6 days of ...</td>\n",
       "      <td>[nasa, confirms, earth, will, experience, 6, d...</td>\n",
       "      <td>thousands of people have been duped by a fake ...</td>\n",
       "      <td>[thousand, of, people, have, been, duped, by, ...</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Accused Boston Marathon Bomber Severely Injure...</td>\n",
       "      <td>A British fighter who travelled to Iraq to sto...</td>\n",
       "      <td>962</td>\n",
       "      <td>accused boston marathon bomber severely injure...</td>\n",
       "      <td>[accused, boston, marathon, bomber, severely, ...</td>\n",
       "      <td>a british fighter who travelled to iraq to sto...</td>\n",
       "      <td>[a, british, fighter, who, travelled, to, iraq...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Identity of ISIS terrorist known as 'Jihadi Jo...</td>\n",
       "      <td>Adding to Apple's iOS 8 launch troubles, a rep...</td>\n",
       "      <td>2033</td>\n",
       "      <td>identity of isis terrorist known as jihadi joh...</td>\n",
       "      <td>[identity, of, isi, terrorist, known, a, jihad...</td>\n",
       "      <td>adding to apple s ios 8 launch troubles a repo...</td>\n",
       "      <td>[adding, to, apple, s, io, 8, launch, trouble,...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  \\\n",
       "0  Police find mass graves with at least '15 bodi...   \n",
       "2  Christian Bale passes on role of Steve Jobs, a...   \n",
       "5  'Nasa Confirms Earth Will Experience 6 Days of...   \n",
       "6  Accused Boston Marathon Bomber Severely Injure...   \n",
       "7  Identity of ISIS terrorist known as 'Jihadi Jo...   \n",
       "\n",
       "                                         articleBody  Body ID  \\\n",
       "0  Danny Boyle is directing the untitled film\\n\\n...      712   \n",
       "2  30-year-old Moscow resident was hospitalized w...      137   \n",
       "5  Thousands of people have been duped by a fake ...      154   \n",
       "6  A British fighter who travelled to Iraq to sto...      962   \n",
       "7  Adding to Apple's iOS 8 launch troubles, a rep...     2033   \n",
       "\n",
       "                                   ___clean_headline  \\\n",
       "0  police find mass graves with at least 15 bodie...   \n",
       "2  christian bale passes on role of steve jobs ac...   \n",
       "5  nasa confirms earth will experience 6 days of ...   \n",
       "6  accused boston marathon bomber severely injure...   \n",
       "7  identity of isis terrorist known as jihadi joh...   \n",
       "\n",
       "                  ___clean_headline_tokenized_lemmas  \\\n",
       "0  [police, find, mass, graf, with, at, least, 15...   \n",
       "2  [christian, bale, pass, on, role, of, steve, j...   \n",
       "5  [nasa, confirms, earth, will, experience, 6, d...   \n",
       "6  [accused, boston, marathon, bomber, severely, ...   \n",
       "7  [identity, of, isi, terrorist, known, a, jihad...   \n",
       "\n",
       "                                       ___clean_body  \\\n",
       "0  danny boyle is directing the untitled film set...   \n",
       "2  30 year old moscow resident was hospitalized w...   \n",
       "5  thousands of people have been duped by a fake ...   \n",
       "6  a british fighter who travelled to iraq to sto...   \n",
       "7  adding to apple s ios 8 launch troubles a repo...   \n",
       "\n",
       "                      ___clean_body_tokenized_lemmas  bin_count  \\\n",
       "0  [danny, boyle, is, directing, the, untitled, f...          2   \n",
       "2  [30, year, old, moscow, resident, wa, hospital...          5   \n",
       "5  [thousand, of, people, have, been, duped, by, ...         17   \n",
       "6  [a, british, fighter, who, travelled, to, iraq...          4   \n",
       "7  [adding, to, apple, s, io, 8, launch, trouble,...          2   \n",
       "\n",
       "   bin_count_early  bin_count_stopless           ...            \\\n",
       "0                0                   0           ...             \n",
       "2                4                   1           ...             \n",
       "5               15                  14           ...             \n",
       "6                1                   1           ...             \n",
       "7                1                   0           ...             \n",
       "\n",
       "   chargram_2_first_hits  chargram_8_hits  chargram_8_early_hits  \\\n",
       "0                      0                0                      0   \n",
       "2                      1                0                      0   \n",
       "5                      3                0                      0   \n",
       "6                      2                0                      0   \n",
       "7                      2                0                      0   \n",
       "\n",
       "   chargram_8_first_hits  chargram_4_hits  chargram_4_early_hits  \\\n",
       "0                      0                0                      0   \n",
       "2                      0                0                      0   \n",
       "5                      0                0                      0   \n",
       "6                      0                0                      0   \n",
       "7                      0                0                      0   \n",
       "\n",
       "   chargram_4_first_hits  chargram_16_hits  chargram_16_early_hits  \\\n",
       "0                      0                 0                       0   \n",
       "2                      0                 0                       0   \n",
       "5                      0                 0                       0   \n",
       "6                      0                 0                       0   \n",
       "7                      0                 0                       0   \n",
       "\n",
       "   chargram_16_first_hits  \n",
       "0                       0  \n",
       "2                       0  \n",
       "5                       0  \n",
       "6                       0  \n",
       "7                       0  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>articleBody</th>\n",
       "      <th>Body ID</th>\n",
       "      <th>___clean_headline</th>\n",
       "      <th>___clean_headline_tokenized_lemmas</th>\n",
       "      <th>___clean_body</th>\n",
       "      <th>___clean_body_tokenized_lemmas</th>\n",
       "      <th>bin_count</th>\n",
       "      <th>bin_count_early</th>\n",
       "      <th>bin_count_stopless</th>\n",
       "      <th>...</th>\n",
       "      <th>chargram_2_first_hits</th>\n",
       "      <th>chargram_8_hits</th>\n",
       "      <th>chargram_8_early_hits</th>\n",
       "      <th>chargram_8_first_hits</th>\n",
       "      <th>chargram_4_hits</th>\n",
       "      <th>chargram_4_early_hits</th>\n",
       "      <th>chargram_4_first_hits</th>\n",
       "      <th>chargram_16_hits</th>\n",
       "      <th>chargram_16_early_hits</th>\n",
       "      <th>chargram_16_first_hits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hundreds of Palestinians flee floods in Gaza a...</td>\n",
       "      <td>Hundreds of Palestinians were evacuated from t...</td>\n",
       "      <td>158</td>\n",
       "      <td>hundreds of palestinians flee floods in gaza a...</td>\n",
       "      <td>[hundred, of, palestinian, flee, flood, in, ga...</td>\n",
       "      <td>hundreds of palestinians were evacuated from t...</td>\n",
       "      <td>[hundred, of, palestinian, were, evacuated, fr...</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HBO and Apple in Talks for $15/Month Apple TV ...</td>\n",
       "      <td>(Reuters) - A Canadian soldier was shot at the...</td>\n",
       "      <td>1034</td>\n",
       "      <td>hbo and apple in talks for 15 month apple tv s...</td>\n",
       "      <td>[hbo, and, apple, in, talk, for, 15, month, ap...</td>\n",
       "      <td>reuters a canadian soldier was shot at the can...</td>\n",
       "      <td>[reuters, a, canadian, soldier, wa, shot, at, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Spider burrowed through tourist's stomach and ...</td>\n",
       "      <td>Fear not arachnophobes, the story of Bunbury's...</td>\n",
       "      <td>1923</td>\n",
       "      <td>spider burrowed through tourist s stomach and ...</td>\n",
       "      <td>[spider, burrowed, through, tourist, s, stomac...</td>\n",
       "      <td>fear not arachnophobes the story of bunbury s ...</td>\n",
       "      <td>[fear, not, arachnophobes, the, story, of, bun...</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Kidnapped Nigerian schoolgirls: Government cla...</td>\n",
       "      <td>No one has died more times than Fidel Castro.\\...</td>\n",
       "      <td>1003</td>\n",
       "      <td>kidnapped nigerian schoolgirls government clai...</td>\n",
       "      <td>[kidnapped, nigerian, schoolgirl, government, ...</td>\n",
       "      <td>no one has died more times than fidel castro b...</td>\n",
       "      <td>[no, one, ha, died, more, time, than, fidel, c...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>No, that high school kid didn't make $72 milli...</td>\n",
       "      <td>The video was one of those viral sensations th...</td>\n",
       "      <td>2132</td>\n",
       "      <td>no that high school kid didn t make 72 million...</td>\n",
       "      <td>[no, that, high, school, kid, didn, t, make, 7...</td>\n",
       "      <td>the video was one of those viral sensations th...</td>\n",
       "      <td>[the, video, wa, one, of, those, viral, sensat...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Headline  \\\n",
       "1   Hundreds of Palestinians flee floods in Gaza a...   \n",
       "3   HBO and Apple in Talks for $15/Month Apple TV ...   \n",
       "4   Spider burrowed through tourist's stomach and ...   \n",
       "12  Kidnapped Nigerian schoolgirls: Government cla...   \n",
       "13  No, that high school kid didn't make $72 milli...   \n",
       "\n",
       "                                          articleBody  Body ID  \\\n",
       "1   Hundreds of Palestinians were evacuated from t...      158   \n",
       "3   (Reuters) - A Canadian soldier was shot at the...     1034   \n",
       "4   Fear not arachnophobes, the story of Bunbury's...     1923   \n",
       "12  No one has died more times than Fidel Castro.\\...     1003   \n",
       "13  The video was one of those viral sensations th...     2132   \n",
       "\n",
       "                                    ___clean_headline  \\\n",
       "1   hundreds of palestinians flee floods in gaza a...   \n",
       "3   hbo and apple in talks for 15 month apple tv s...   \n",
       "4   spider burrowed through tourist s stomach and ...   \n",
       "12  kidnapped nigerian schoolgirls government clai...   \n",
       "13  no that high school kid didn t make 72 million...   \n",
       "\n",
       "                   ___clean_headline_tokenized_lemmas  \\\n",
       "1   [hundred, of, palestinian, flee, flood, in, ga...   \n",
       "3   [hbo, and, apple, in, talk, for, 15, month, ap...   \n",
       "4   [spider, burrowed, through, tourist, s, stomac...   \n",
       "12  [kidnapped, nigerian, schoolgirl, government, ...   \n",
       "13  [no, that, high, school, kid, didn, t, make, 7...   \n",
       "\n",
       "                                        ___clean_body  \\\n",
       "1   hundreds of palestinians were evacuated from t...   \n",
       "3   reuters a canadian soldier was shot at the can...   \n",
       "4   fear not arachnophobes the story of bunbury s ...   \n",
       "12  no one has died more times than fidel castro b...   \n",
       "13  the video was one of those viral sensations th...   \n",
       "\n",
       "                       ___clean_body_tokenized_lemmas  bin_count  \\\n",
       "1   [hundred, of, palestinian, were, evacuated, fr...         10   \n",
       "3   [reuters, a, canadian, soldier, wa, shot, at, ...          3   \n",
       "4   [fear, not, arachnophobes, the, story, of, bun...          9   \n",
       "12  [no, one, ha, died, more, time, than, fidel, c...          2   \n",
       "13  [the, video, wa, one, of, those, viral, sensat...          4   \n",
       "\n",
       "    bin_count_early  bin_count_stopless           ...            \\\n",
       "1                 7                   7           ...             \n",
       "3                 3                   0           ...             \n",
       "4                 5                   4           ...             \n",
       "12                2                   0           ...             \n",
       "13                2                   2           ...             \n",
       "\n",
       "    chargram_2_first_hits  chargram_8_hits  chargram_8_early_hits  \\\n",
       "1                       3                0                      0   \n",
       "3                       1                0                      0   \n",
       "4                       4                0                      0   \n",
       "12                      3                0                      0   \n",
       "13                      1                0                      0   \n",
       "\n",
       "    chargram_8_first_hits  chargram_4_hits  chargram_4_early_hits  \\\n",
       "1                       0                0                      0   \n",
       "3                       0                0                      0   \n",
       "4                       0                0                      0   \n",
       "12                      0                0                      0   \n",
       "13                      0                0                      0   \n",
       "\n",
       "    chargram_4_first_hits  chargram_16_hits  chargram_16_early_hits  \\\n",
       "1                       0                 0                       0   \n",
       "3                       0                 0                       0   \n",
       "4                       0                 0                       0   \n",
       "12                      0                 0                       0   \n",
       "13                      0                 0                       0   \n",
       "\n",
       "    chargram_16_first_hits  \n",
       "1                        0  \n",
       "3                        0  \n",
       "4                        0  \n",
       "12                       0  \n",
       "13                       0  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gui/anaconda3/envs/nlu4/lib/python3.6/site-packages/pandas/core/generic.py:1993: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->['Headline', 'articleBody', '___clean_headline', '___clean_headline_tokenized_lemmas', '___clean_body', '___clean_body_tokenized_lemmas']]\n",
      "\n",
      "  return pytables.to_hdf(path_or_buf, key, self, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "X_train.to_hdf('X_train_disjoint-allfeatures.h5','df')\n",
    "X_dev.to_hdf('X_dev_disjoint-allfeatures.h5','df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodies_test = pd.read_csv(os.path.join(data_path, \"competition_test_bodies.csv\"), header=0)\n",
    "headlines_test = pd.read_csv(os.path.join(data_path, \"competition_test_stances.csv\"), header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_test = headlines_test.merge(bodies_test, on=[\"Body ID\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body ID</th>\n",
       "      <th>Stance</th>\n",
       "      <th>articleBody</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ferguson riots: Pregnant woman loses eye after...</td>\n",
       "      <td>2008</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>A RESPECTED senior French police officer inves...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crazy Conservatives Are Sure a Gitmo Detainee ...</td>\n",
       "      <td>1550</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>Dave Morin's social networking company Path is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Russian Guy Says His Justin Bieber Ringtone ...</td>\n",
       "      <td>2</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>A bereaved Afghan mother took revenge on the T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zombie Cat: Buried Kitty Believed Dead, Meows ...</td>\n",
       "      <td>1793</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>Hewlett-Packard is officially splitting in two...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Argentina's President Adopts Boy to End Werewo...</td>\n",
       "      <td>37</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>An airline passenger headed to Dallas was remo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  Body ID     Stance  \\\n",
       "0  Ferguson riots: Pregnant woman loses eye after...     2008  unrelated   \n",
       "1  Crazy Conservatives Are Sure a Gitmo Detainee ...     1550  unrelated   \n",
       "2  A Russian Guy Says His Justin Bieber Ringtone ...        2  unrelated   \n",
       "3  Zombie Cat: Buried Kitty Believed Dead, Meows ...     1793  unrelated   \n",
       "4  Argentina's President Adopts Boy to End Werewo...       37  unrelated   \n",
       "\n",
       "                                         articleBody  \n",
       "0  A RESPECTED senior French police officer inves...  \n",
       "1  Dave Morin's social networking company Path is...  \n",
       "2  A bereaved Afghan mother took revenge on the T...  \n",
       "3  Hewlett-Packard is officially splitting in two...  \n",
       "4  An airline passenger headed to Dallas was remo...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xy_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = Xy_test[[\"Stance\", \"Body ID\"]]\n",
    "X_test = Xy_test[[\"Headline\", \"articleBody\", \"Body ID\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.to_hdf('y_TEST_disjoint.h5','df')\n",
    "del y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding cached columns...\n",
      "Adding co-occurrences...\n",
      "Adding word_overlap_features....\n",
      "Adding refuting_features....\n",
      "Adding polarity_features....\n",
      "Adding ngrams...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "130b77f2c32f47ebbc4643f3fa5944c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding chargrams...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638c94303f0c4723ad4b1896f5b4a969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 59.2 s, sys: 4min 33s, total: 5min 32s\n",
      "Wall time: 7min 5s\n"
     ]
    }
   ],
   "source": [
    "%time X_test = gen_all_features(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gui/anaconda3/envs/nlu4/lib/python3.6/site-packages/pandas/core/generic.py:1993: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->['Headline', 'articleBody', '___clean_headline', '___clean_headline_tokenized_lemmas', '___clean_body', '___clean_body_tokenized_lemmas']]\n",
      "\n",
      "  return pytables.to_hdf(path_or_buf, key, self, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "X_test.to_hdf('X_TEST-allfeatures.h5','df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
