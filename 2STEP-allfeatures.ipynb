{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/gui/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gui/anaconda3/envs/nlu4/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import os\n",
    "from tqdm import tqdm, trange, tqdm_notebook\n",
    "import re\n",
    "import nltk\n",
    "from multiprocessing import cpu_count, Pool\n",
    "from functools import partial\n",
    "import ipywidgets\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from rewrite.scorer import score_4class\n",
    "import utils # utils from CS224U\n",
    "from scipy.spatial import distance\n",
    "import random\n",
    "tqdm.pandas()\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.sentiment import vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_str_cols(df): # df should be X, e.g. X_train or X_dev\n",
    "    del df[\"articleBody\"]\n",
    "    del df[\"Headline\"]\n",
    "    for col_name in df.columns:\n",
    "        if \"___\" == col_name[0:3]:\n",
    "            del df[col_name]\n",
    "            \n",
    "def print_reports(preds, actual):\n",
    "    print(classification_report(actual, preds))\n",
    "    score, max_score = score_4class(actual, preds)\n",
    "    print(\"Weighted accuracy: \"+str(score/max_score)+\" (\"+str(score)+\" out of \"+str(max_score)+\")\")\n",
    "\n",
    "def print_feature_importances(model, df):\n",
    "    feat_imp = model.feature_importances_\n",
    "    indices = np.argsort(feat_imp)[::-1]\n",
    "    for ii in indices:\n",
    "        print(df.columns[ii]+\": \"+str(feat_imp[ii]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoModel:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        #self.make_other_sets()\n",
    "        \n",
    "    def make_other_sets(self):\n",
    "        Xy_train = pd.concat([X_train, y_train], axis=1)\n",
    "        Xy_train, Xy_dev = sklearn.model_selection.train_test_split(Xy_train, test_size=0.2, random_state=42, shuffle=True)\n",
    "        \n",
    "        Xy_train_1 = Xy_train.copy()\n",
    "        Xy_train_1.loc[Xy_train_1[\"Stance\"] != \"unrelated\", 'Stance'] = \"related\"\n",
    "        self.y_train_1 = Xy_train_1[\"Stance\"]\n",
    "        self.X_train_1 = Xy_train_1.drop(\"Stance\", axis=1)\n",
    "        \n",
    "        Xy_dev_1 = Xy_dev.copy()\n",
    "        Xy_dev_1.loc[Xy_dev_1[\"Stance\"] != \"unrelated\", 'Stance'] = \"related\"\n",
    "        self.y_dev_1 = Xy_dev_1[\"Stance\"]\n",
    "        self.X_dev_1 = Xy_dev_1.drop(\"Stance\", axis=1)\n",
    "        \n",
    "        Xy_train_2 = Xy_train[Xy_train[\"Stance\"] != \"unrelated\"]\n",
    "        self.y_train_2 = Xy_train_2[\"Stance\"]\n",
    "        self.X_train_2 = Xy_train_2.drop(\"Stance\", axis=1)\n",
    "\n",
    "        Xy_dev_2 = Xy_dev[Xy_dev[\"Stance\"] != \"unrelated\"]\n",
    "        self.y_dev_2 = Xy_dev_2[\"Stance\"]\n",
    "        self.X_dev_2 = Xy_dev_2.drop(\"Stance\", axis=1)\n",
    "        \n",
    "        del_str_cols(self.X_train_1)\n",
    "        del_str_cols(self.X_dev_1)\n",
    "        del_str_cols(self.X_train_2)\n",
    "        del_str_cols(self.X_dev_2)\n",
    "        \n",
    "        return Xy_train.drop(\"Stance\", axis=1), Xy_train[\"Stance\"], Xy_dev.drop(\"Stance\", axis=1), Xy_dev[\"Stance\"]\n",
    "        \n",
    "    def fit(self):\n",
    "        ## CLASSIFIER 1 - RELATED/UNRELATED\n",
    "        self.mod1 = GradientBoostingClassifier(n_estimators=200, random_state=14128, verbose=True)\n",
    "        self.mod1.fit(self.X_train_1, self.y_train_1)\n",
    "        \n",
    "        ## CLASSIFIER 2 - 3-class\n",
    "        self.mod2 = GradientBoostingClassifier(n_estimators=200, random_state=14128, verbose=True)\n",
    "        self.mod2.fit(self.X_train_2, self.y_train_2)\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        #if X is None:\n",
    "        #    X = self.X_dev_1\n",
    "        del_str_cols(X)\n",
    "        preds_1 = self.mod1.predict(X)\n",
    "        preds_2 = self.mod2.predict(X) # note X_dev_1\n",
    "        \n",
    "        new_preds = preds_1.copy()\n",
    "        for ii in range(preds_1.shape[0]):\n",
    "            if preds_1[ii] == \"related\":\n",
    "                new_preds[ii] = preds_2[ii]\n",
    "            else:\n",
    "                new_preds[ii] = \"unrelated\"\n",
    "        return new_preds\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count, Pool\n",
    "cores = cpu_count() \n",
    "partitions = cores\n",
    "def parallelize(data, func):\n",
    "    data_split = np.array_split(data, partitions)\n",
    "    pool = Pool(cores)\n",
    "    data = pd.concat(pool.map(func, data_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ADD OUR FEATURES\n",
    "# START SENTIMENT ANALYSIS\n",
    "def vader_pol_helper(df):\n",
    "    return df.apply(lambda hl: pd.Series(sid.polarity_scores(hl)))\n",
    "sid = vader.SentimentIntensityAnalyzer() # global scope for parallelization\n",
    "def add_vader_sent(X_df):\n",
    "    def vader_polarity_scores(df, text_col_name, col_name_prefix):\n",
    "        pol_scores = parallelize(df[text_col_name], vader_pol_helper)\n",
    "        #pol_scores = df[text_col_name].progress_apply(lambda hl: pd.Series(sid.polarity_scores(hl)))\n",
    "        cols = pol_scores.columns\n",
    "        new_cols = []\n",
    "        for col_name in cols:\n",
    "            new_cols.append(\"vader_\"+col_name_prefix+\"_\"+col_name)\n",
    "        pol_scores.columns = new_cols\n",
    "        return pol_scores\n",
    "\n",
    "    vader_hl_df = vader_polarity_scores(X_df, \"Headline\", \"hl\")\n",
    "    vader_body_df = vader_polarity_scores(X_df, \"articleBody\", \"body\")\n",
    "    X_df = pd.concat([X_df, vader_hl_df, vader_body_df], axis=1)\n",
    "    return X_df\n",
    "\n",
    "# END SENTIMENT ANALYSIS\n",
    "### GLOVE ####\n",
    "glove_dim = 200\n",
    "glove_src = os.path.join(\"GloVe\", 'glove.6B.'+str(glove_dim)+'d.txt')\n",
    "GLOVE = utils.glove2dict(glove_src)\n",
    "def text_to_mean_vec_ignore_unk(text, w2v=GLOVE, dim=glove_dim):\n",
    "    vec = np.zeros(dim)\n",
    "    num_added = 0\n",
    "    for word in text:\n",
    "        if word in w2v:\n",
    "            vec += w2v[word]\n",
    "            num_added += 1\n",
    "    if num_added > 0:\n",
    "        return vec/num_added\n",
    "    else:\n",
    "        return np.array([random.uniform(-0.5, 0.5) for i in range(glove_dim)])\n",
    "def get_glove_cos_dist_hl_body(row):\n",
    "    hl = row[\"___clean_headline_tokenized_lemmas\"]\n",
    "    body = row[\"___clean_body_tokenized_lemmas\"]\n",
    "    hl_vec = text_to_mean_vec_ignore_unk(hl)\n",
    "    body_vec = text_to_mean_vec_ignore_unk(body)\n",
    "    cosine_dist = distance.cosine(hl_vec, body_vec) # cosine() from scipy\n",
    "    return cosine_dist\n",
    "\n",
    "def get_verbs(text):\n",
    "    verbs = [token for token, pos in nltk.pos_tag(text) if pos.startswith('VB')]\n",
    "    verbs_sentence = ' '.join(word[0] for word in verbs)\n",
    "    return verbs_sentence\n",
    " \n",
    "def get_verb_glove_cos_dist_hl_body(row):\n",
    "    hl = row[\"___clean_headline_tokenized_lemmas\"]\n",
    "    body = row[\"___clean_body_tokenized_lemmas\"]\n",
    "    \n",
    "    hl_verbs = get_verbs(hl)\n",
    "    body_verbs = get_verbs(body)\n",
    "    \n",
    "    hl_vec = text_to_mean_vec_ignore_unk(hl_verbs)\n",
    "    body_vec = text_to_mean_vec_ignore_unk(body_verbs)\n",
    "    cosine_dist = distance.cosine(hl_vec, body_vec) # cosine() from scipy\n",
    "    return cosine_dist\n",
    "\n",
    "def hl_body_glove_cos_dist_helper(X_df):\n",
    "    return X_df[[\"___clean_headline_tokenized_lemmas\", \"___clean_body_tokenized_lemmas\"]].apply(get_glove_cos_dist_hl_body, axis=1)\n",
    "\n",
    "def hl_body_glove_verb_helper(X_df):\n",
    "    return X_df[[\"___clean_headline_tokenized_lemmas\", \"___clean_body_tokenized_lemmas\"]].apply(get_verb_glove_cos_dist_hl_body, axis=1)\n",
    "\n",
    "### END GLOVE CODE BLOCK\n",
    "def add_all_features(X_df, parallel=True):\n",
    "    print(\"Adding glove feature #1...\", flush=True)\n",
    "    X_df[\"hl_body_glove_\"+str(glove_dim)+\"_cos_dist\"] = parallelize(X_df, hl_body_glove_cos_dist_helper)\n",
    "    print(\"Adding glove feature #2...\", flush=True)\n",
    "    X_df[\"hl_body_verb_glove_\"+str(glove_dim)+\"_cos_dist\"] = parallelize(X_df, hl_body_glove_verb_helper)\n",
    "    print(\"Adding VADER sentiment...\", flush=True)\n",
    "    X_df = add_vader_sent(X_df)\n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_hdf(\"X_train_full_allfeatures-NOLABEL.h5\", key=\"df\")\n",
    "y_train = pd.read_hdf(\"y_train_full.h5\", key=\"df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding glove feature #1...\n",
      "Adding glove feature #2...\n",
      "Adding VADER sentiment...\n",
      "CPU times: user 11.1 s, sys: 10.9 s, total: 22 s\n",
      "Wall time: 7min 22s\n"
     ]
    }
   ],
   "source": [
    "%time X_train = add_all_features(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49972, 59)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsm = TwoModel(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39977, 59)\n",
      "(39977,)\n",
      "(9995, 59)\n",
      "(9995,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_dev, y_dev = tsm.make_other_sets()\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_dev.shape)\n",
    "print(y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>articleBody</th>\n",
       "      <th>___clean_headline</th>\n",
       "      <th>___clean_headline_tokenized_lemmas</th>\n",
       "      <th>___clean_body</th>\n",
       "      <th>___clean_body_tokenized_lemmas</th>\n",
       "      <th>bin_count</th>\n",
       "      <th>bin_count_early</th>\n",
       "      <th>bin_count_stopless</th>\n",
       "      <th>word_overlap_features</th>\n",
       "      <th>...</th>\n",
       "      <th>hl_body_glove_200_cos_dist</th>\n",
       "      <th>hl_body_verb_glove_200_cos_dist</th>\n",
       "      <th>vader_hl_neg</th>\n",
       "      <th>vader_hl_neu</th>\n",
       "      <th>vader_hl_pos</th>\n",
       "      <th>vader_hl_compound</th>\n",
       "      <th>vader_body_neg</th>\n",
       "      <th>vader_body_neu</th>\n",
       "      <th>vader_body_pos</th>\n",
       "      <th>vader_body_compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45131</th>\n",
       "      <td>Insurgents killed in Nigeria despite alleged t...</td>\n",
       "      <td>Anna Wintour, editor-in-chief on Vogue magazin...</td>\n",
       "      <td>insurgents killed in nigeria despite alleged t...</td>\n",
       "      <td>[insurgent, killed, in, nigeria, despite, alle...</td>\n",
       "      <td>anna wintour editor in chief on vogue magazine...</td>\n",
       "      <td>[anna, wintour, editor, in, chief, on, vogue, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009756</td>\n",
       "      <td>...</td>\n",
       "      <td>0.349139</td>\n",
       "      <td>0.322180</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.6705</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.8214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48011</th>\n",
       "      <td>New Audio Reveals Pause in Gunfire When Michae...</td>\n",
       "      <td>SEVEN girls, aged 13 to 15, have fallen pregna...</td>\n",
       "      <td>new audio reveals pause in gunfire when michae...</td>\n",
       "      <td>[new, audio, reveals, pause, in, gunfire, when...</td>\n",
       "      <td>seven girls aged 13 to 15 have fallen pregnant...</td>\n",
       "      <td>[seven, girl, aged, 13, to, 15, have, fallen, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180693</td>\n",
       "      <td>0.178216</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.9393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26530</th>\n",
       "      <td>Iraqi Official Dismisses ‘Unfounded’ Reports T...</td>\n",
       "      <td>Everyone's been waiting years and years for a ...</td>\n",
       "      <td>iraqi official dismisses unfounded reports tha...</td>\n",
       "      <td>[iraqi, official, dismisses, unfounded, report...</td>\n",
       "      <td>everyone s been waiting years and years for a ...</td>\n",
       "      <td>[everyone, s, been, waiting, year, and, year, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.018634</td>\n",
       "      <td>...</td>\n",
       "      <td>0.292790</td>\n",
       "      <td>0.254490</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0516</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.9289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30200</th>\n",
       "      <td>Nigeria says it has deal with Boko Haram to re...</td>\n",
       "      <td>WASHINGTON — A U.S. Republican senator and fre...</td>\n",
       "      <td>nigeria says it has deal with boko haram to re...</td>\n",
       "      <td>[nigeria, say, it, ha, deal, with, boko, haram...</td>\n",
       "      <td>washington a u s republican senator and freque...</td>\n",
       "      <td>[washington, a, u, s, republican, senator, and...</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174129</td>\n",
       "      <td>0.097080</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.088</td>\n",
       "      <td>-0.6236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7195</th>\n",
       "      <td>ISIL allegedly kills US journalist in video</td>\n",
       "      <td>Absolutely awful news. Media are reporting tha...</td>\n",
       "      <td>isil allegedly kills us journalist in video</td>\n",
       "      <td>[isil, allegedly, kill, u, journalist, in, video]</td>\n",
       "      <td>absolutely awful news media are reporting that...</td>\n",
       "      <td>[absolutely, awful, news, medium, are, reporti...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214252</td>\n",
       "      <td>1.008478</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.5423</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.5095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Headline  \\\n",
       "45131  Insurgents killed in Nigeria despite alleged t...   \n",
       "48011  New Audio Reveals Pause in Gunfire When Michae...   \n",
       "26530  Iraqi Official Dismisses ‘Unfounded’ Reports T...   \n",
       "30200  Nigeria says it has deal with Boko Haram to re...   \n",
       "7195         ISIL allegedly kills US journalist in video   \n",
       "\n",
       "                                             articleBody  \\\n",
       "45131  Anna Wintour, editor-in-chief on Vogue magazin...   \n",
       "48011  SEVEN girls, aged 13 to 15, have fallen pregna...   \n",
       "26530  Everyone's been waiting years and years for a ...   \n",
       "30200  WASHINGTON — A U.S. Republican senator and fre...   \n",
       "7195   Absolutely awful news. Media are reporting tha...   \n",
       "\n",
       "                                       ___clean_headline  \\\n",
       "45131  insurgents killed in nigeria despite alleged t...   \n",
       "48011  new audio reveals pause in gunfire when michae...   \n",
       "26530  iraqi official dismisses unfounded reports tha...   \n",
       "30200  nigeria says it has deal with boko haram to re...   \n",
       "7195         isil allegedly kills us journalist in video   \n",
       "\n",
       "                      ___clean_headline_tokenized_lemmas  \\\n",
       "45131  [insurgent, killed, in, nigeria, despite, alle...   \n",
       "48011  [new, audio, reveals, pause, in, gunfire, when...   \n",
       "26530  [iraqi, official, dismisses, unfounded, report...   \n",
       "30200  [nigeria, say, it, ha, deal, with, boko, haram...   \n",
       "7195   [isil, allegedly, kill, u, journalist, in, video]   \n",
       "\n",
       "                                           ___clean_body  \\\n",
       "45131  anna wintour editor in chief on vogue magazine...   \n",
       "48011  seven girls aged 13 to 15 have fallen pregnant...   \n",
       "26530  everyone s been waiting years and years for a ...   \n",
       "30200  washington a u s republican senator and freque...   \n",
       "7195   absolutely awful news media are reporting that...   \n",
       "\n",
       "                          ___clean_body_tokenized_lemmas  bin_count  \\\n",
       "45131  [anna, wintour, editor, in, chief, on, vogue, ...          3   \n",
       "48011  [seven, girl, aged, 13, to, 15, have, fallen, ...          2   \n",
       "26530  [everyone, s, been, waiting, year, and, year, ...          2   \n",
       "30200  [washington, a, u, s, republican, senator, and...          7   \n",
       "7195   [absolutely, awful, news, medium, are, reporti...          2   \n",
       "\n",
       "       bin_count_early  bin_count_stopless  word_overlap_features  \\\n",
       "45131                3                   1               0.009756   \n",
       "48011                1                   0               0.021739   \n",
       "26530                1                   0               0.018634   \n",
       "30200                4                   3               0.025000   \n",
       "7195                 2                   1               0.086957   \n",
       "\n",
       "              ...           hl_body_glove_200_cos_dist  \\\n",
       "45131         ...                             0.349139   \n",
       "48011         ...                             0.180693   \n",
       "26530         ...                             0.292790   \n",
       "30200         ...                             0.174129   \n",
       "7195          ...                             0.214252   \n",
       "\n",
       "       hl_body_verb_glove_200_cos_dist  vader_hl_neg  vader_hl_neu  \\\n",
       "45131                         0.322180         0.333         0.667   \n",
       "48011                         0.178216         0.000         1.000   \n",
       "26530                         0.254490         0.118         0.882   \n",
       "30200                         0.097080         0.000         1.000   \n",
       "7195                          1.008478         0.368         0.632   \n",
       "\n",
       "       vader_hl_pos  vader_hl_compound  vader_body_neg  vader_body_neu  \\\n",
       "45131           0.0            -0.6705           0.053           0.875   \n",
       "48011           0.0             0.0000           0.120           0.880   \n",
       "26530           0.0            -0.0516           0.009           0.929   \n",
       "30200           0.0             0.0000           0.090           0.823   \n",
       "7195            0.0            -0.5423           0.162           0.838   \n",
       "\n",
       "       vader_body_pos  vader_body_compound  \n",
       "45131           0.073               0.8214  \n",
       "48011           0.000              -0.9393  \n",
       "26530           0.063               0.9289  \n",
       "30200           0.088              -0.6236  \n",
       "7195            0.000              -0.5095  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.0244           11.69s\n",
      "         2           0.9193           11.26s\n",
      "         3           0.8355           10.85s\n",
      "         4           0.7672           10.98s\n",
      "         5           0.7104           10.89s\n",
      "         6           0.6616           10.79s\n",
      "         7           0.6199           10.62s\n",
      "         8           0.5840           10.53s\n",
      "         9           0.5530           10.44s\n",
      "        10           0.5263           10.29s\n",
      "        20           0.3712            9.69s\n",
      "        30           0.3183            8.96s\n",
      "        40           0.2869            8.48s\n",
      "        50           0.2705            8.04s\n",
      "        60           0.2579            7.42s\n",
      "        70           0.2480            6.89s\n",
      "        80           0.2414            6.40s\n",
      "        90           0.2350            5.89s\n",
      "       100           0.2300            5.38s\n",
      "       200           0.1958            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        9825.4960           12.04s\n",
      "         2        9537.6722           10.72s\n",
      "         3        9291.4585           11.11s\n",
      "         4        9082.2030           10.70s\n",
      "         5        8907.5404           10.47s\n",
      "         6        8748.1160           10.20s\n",
      "         7        8611.2626            9.97s\n",
      "         8        8489.4987            9.75s\n",
      "         9        8378.3609            9.71s\n",
      "        10        8288.3013            9.60s\n",
      "        20        7738.4393            8.75s\n",
      "        30        7473.9541            8.01s\n",
      "        40        7298.9233            7.29s\n",
      "        50        7144.4513            6.73s\n",
      "        60        7010.2696            6.14s\n",
      "        70        6876.3182            5.75s\n",
      "        80        6751.9998            5.26s\n",
      "        90        6648.3896            4.77s\n",
      "       100        6549.4795            4.33s\n",
      "       200        5714.6407            0.00s\n"
     ]
    }
   ],
   "source": [
    "tsm.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      agree       0.69      0.33      0.44       703\n",
      "   disagree       0.70      0.12      0.20       180\n",
      "    discuss       0.71      0.89      0.79      1779\n",
      "  unrelated       0.97      0.98      0.98      7333\n",
      "\n",
      "avg / total       0.90      0.90      0.89      9995\n",
      "\n",
      "Weighted accuracy: 0.8445025304488071 (3796.25 out of 4495.25)\n"
     ]
    }
   ],
   "source": [
    "preds = tsm.predict(X_dev)\n",
    "print_reports(preds, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_hdf(\"X_TEST_full_allfeatures-NOLABEL.h5\", key=\"df\")\n",
    "y_test = pd.read_hdf(\"y_TEST_full.h5\", key=\"df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding glove feature #1...\n",
      "Adding glove feature #2...\n",
      "Adding VADER sentiment...\n"
     ]
    }
   ],
   "source": [
    "X_test = add_all_features(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      agree       0.45      0.20      0.28      1903\n",
      "   disagree       0.39      0.01      0.02       697\n",
      "    discuss       0.62      0.79      0.69      4464\n",
      "  unrelated       0.95      0.97      0.96     18349\n",
      "\n",
      "avg / total       0.84      0.86      0.84     25413\n",
      "\n",
      "Weighted accuracy: 0.7661838858491579 (8927.0 out of 11651.25)\n"
     ]
    }
   ],
   "source": [
    "preds = tsm.predict(X_test)\n",
    "print_reports(preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = sklearn.ensemble.RandomForestClassifier(n_estimators=200, n_jobs=8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = crf.predict(X_dev)\n",
    "print(classification_report(y_dev, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, preds, labels=[\"agree\", \"disagree\", \"discuss\"], sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
